{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10WdpyFVm5G8J5AJrHPelKDpmooAreyPw",
      "authorship_tag": "ABX9TyOYqCI3/z7mY7urdWFFqbUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-byrne/MADSmilestone2/blob/main/Supervised_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6L-qwlfHPf"
      },
      "source": [
        "# Supervised Learning: Clock Drawing Image Classification with Convolutional Neural Networks\n",
        "### Stacey Beck and Ian Byrne\n",
        "\n",
        "- Split data into sets of Training (x = image arrays ; y = labels), Test (~10% image arrays), and Validation (~10% of the Training). \n",
        "- Build CNN using Pytorch for Training and Test:\n",
        "  - Specify CUDA\n",
        "  - 2D convolution, Normalization (for faster training), Non-linear Activation Function (ex. RELU), Max Pooling (downsampling to reduce learned parameters).\n",
        "  - Define Layers \n",
        "  - Build Forward and backward pass\n",
        "  - Define optimizer (due to many - deep - nodes) ex) ADAM\n",
        "  - Calculate Loss (BCE)\n",
        "  - Calculate Accuracy, Precision, Recall (Confusion Matrix)\n",
        "  - Plot ROC and print Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoct7Or8ezZq",
        "outputId": "8f6a1097-2aab-438b-8e48-53bbfdd0058c"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/ian-byrne/MADSmilestone2.git\n",
        "\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2\n",
        "\n",
        "# List repo contents\n",
        "#!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MADSmilestone2'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 382, done.\u001b[K\n",
            "remote: Counting objects: 100% (382/382), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 382 (delta 210), reused 143 (delta 60), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (382/382), 3.02 MiB | 4.81 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n",
            "/content/MADSmilestone2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtH9tfFdQepN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import ast\n",
        "import Images\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM_UbzqNogPC"
      },
      "source": [
        "#!rm -rf /content/MADSmilestone2"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8QJAPeJfGK4"
      },
      "source": [
        "# Read round, id, label dictionary file\n",
        "\n",
        "cust_file = open(\"Data/customLabelDict.txt\", \"r\")\n",
        "#print(cust_file.readline())\n",
        "contents = cust_file.read()\n",
        "dictionary = ast.literal_eval(contents)\n",
        "cust_file.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV8qowHrj5Kk"
      },
      "source": [
        "def get_images_test(id_rounds):\n",
        "    counter = 0\n",
        "    store_images = []\n",
        "    #image_data = []\n",
        "    label_arr = []\n",
        "\n",
        "    for id, array in tqdm(id_rounds.items()):\n",
        "        for value in array:\n",
        "            if counter < 40:\n",
        "                url = 'https://clockimages.s3.us-west-1.amazonaws.com/NHATS_R' + str(\n",
        "                    id) + '_ClockDrawings/' + value[0] + '.tif'\n",
        "                #url = 'https://test-bucket-clockids-aicrowd.s3.us-west-1.amazonaws.com/1_'+ value[0] + '.tif'\n",
        "                # Open files and convert to work with Image in PIL\n",
        "                response = requests.get(url)  # , stream = True)\n",
        "                f = io.BytesIO(response.content)\n",
        "                im_pil = Image.open(f)\n",
        "\n",
        "                # Resize pil image files\n",
        "                #resized = im_pil.resize((im_pil.width // 9, im_pil.height // 9))\n",
        "                resized = im_pil.resize((284,368))\n",
        "                imarray1 = np.array(resized)\n",
        "                \n",
        "                imarray1 = np.array(imarray1).astype(int)\n",
        "                #imarray = np.logical_not(np.array(im)).astype(int) #bool to int, inverts values\n",
        "                #image_data.append(get_coordinates(imarray1))  # , imarray1.shape[0], imarray1.shape[1]))\n",
        "\n",
        "                #Store the np array images into a list\n",
        "                store_images.append(imarray1)\n",
        "\n",
        "                #print(\"round: \", id)\n",
        "\n",
        "                #Visualize the resized images\n",
        "                #viz_image(imarray1, resized, value[0], value[1])\n",
        "                \n",
        "                # Store corresponding labels\n",
        "                label_arr.append(value[1])\n",
        "                \n",
        "                counter += 1\n",
        "\n",
        "    return store_images, label_arr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def viz_image(image, resized, value0, value1):\n",
        "  print(\"shape: \", image.shape)\n",
        "  print(\"spid: \", value0)\n",
        "  print(\"label: \", value1)\n",
        "\n",
        "  # revert\n",
        "  im2 = Image.fromarray(image)\n",
        "  plt.imshow(im2)\n",
        "  plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyUTQpR5kKh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f8b517-9bd9-434a-8c64-e1131da02aa0"
      },
      "source": [
        "x, y = get_images_test(dictionary)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8XCDeIdWYWu",
        "outputId": "62035d76-2122-4f5d-bb1b-71e3aa361012"
      },
      "source": [
        "X = np.array(x)\n",
        "X.shape\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 368, 284)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xGD41P6Q5xfr",
        "outputId": "9d0d746f-2749-4489-d55a-fb1102aaf6b0"
      },
      "source": [
        "plt.imshow(X[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6d678c0390>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jwtXk7qv9-"
      },
      "source": [
        "# Build CNN Model using Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZYMHscwqu7D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKizV2h4lMH"
      },
      "source": [
        "### Building and Training\n",
        "Architecture choices influenced from: \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
        "\n",
        "https://medium.datadriveninvestor.com/five-powerful-cnn-architectures-b939c9ddd57b\n",
        "\n",
        "https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypf_eiS8Eghk"
      },
      "source": [
        "# Define some of the hyperparameters\n",
        "#num_epochs = 100\n",
        "batch_size = 4\n",
        "learning_rate = .001\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1 #2*floor(3/2)\n",
        "#activation function\n",
        "\n",
        "#Get Data\n",
        "# double split to get validation and test\n",
        "ratio_val = 0.1\n",
        "ratio_test = 0.1\n",
        "\n",
        "# Get Train and Test split\n",
        "X_split, X_test, y_split, y_test = train_test_split(X, y, test_size = ratio_test, random_state = 6) \n",
        "\n",
        "# Adjust remaining ratio for even split\n",
        "ratio_remaining = 1 - ratio_test #.11111\n",
        "ratio_val_ad = ratio_val / ratio_remaining \n",
        "\n",
        "# Get Train and Val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_split, y_split, test_size = ratio_val_ad, random_state = 6)\n",
        "\n",
        "# No need to normalize\n",
        "\n",
        "# Zip image data and labels together\n",
        "training_data = [(x, y) for x, y in zip(X_train, y_train)]\n",
        "validation_data = [(x,y) for x, y in zip(X_val, y_val)]\n",
        "test_data = [(x,y) for x, y in zip(X_test, y_test)]\n",
        "\n",
        "\n",
        "# Variables for X - X_train, X_val, X_test\n",
        "#Variables for y - y_train, y_val, y_test\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size = batch_size, shuffle = True) \n",
        "validate_loader = torch.utils.data.DataLoader(validation_data, batch_size = batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "#Labels \n",
        "classes = (0, 1, 2)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "RhabT4VULYLp",
        "outputId": "ee47095f-395a-49cf-c7f0-804e87a7272a"
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "print(images.size())\n",
        "print(images.type())\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images, nrow = 4))\n",
        "# print labels\n",
        "print('Labels:')\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n",
        "\n",
        "#Printing as RGB, just using basic pytorch dataloader likely converting to rbg"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 368, 284])\n",
            "torch.LongTensor\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:\n",
            "    2     2     2     2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "I5xpaSNBrNR5",
        "outputId": "5adcf0c6-803a-4a68-fce6-6f2fb32a0a4b"
      },
      "source": [
        "from torch.nn.modules.activation import ReLU\n",
        "# Set to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    # without considering batch size: Input shape : (None,368, 284, 1) , parameters: (3*3*1*16+16) = 160\n",
        "    self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 16, # one input channel gray scale, 16 filters out\n",
        "                            kernel_size = 3, stride = 1, padding = 1) #Out:(None,368, 284, 16)\n",
        "\n",
        "    #Normalize each output to help w/ faster learning                       \n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) #Out: (None, 184, 142, 16)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 32, \n",
        "                          kernel_size = 3, stride = 1, padding = 1) #Out: (None, 184, 142, 32), params: (3*3*16*32+32) = 4640                         \n",
        "    self.bn2 = nn.BatchNorm2d(32) \n",
        "    self.pool2 = nn.MaxPool2d(2, 2) #Output shape = (None, 92, 71, 32)\n",
        "    # Flatten\n",
        "    self.fc1 = nn.Linear(32*92*71,120)\n",
        "    self.fc2 = nn.Linear(120, 30)\n",
        "    self.fc3 = nn.Linear(30, 3) # left with 3 for the three classes\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool1(self.bn1(F.relu(self.conv1(x))))\n",
        "    x = self.pool2(self.bn2(F.relu(self.conv2(x))))\n",
        "    x = x.view(32*92*71)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "# Create model object \n",
        "model = ConvNet().to(device)\n",
        "for param in model.parameters():\n",
        "  print(str(param.data.numpy().shape)+'\\n')\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer (can use SGD or ADAM)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate) #or ADAM/ momentum\n",
        "acc = 0 # accuracy\n",
        "epoch = 0 # training episodes\n",
        "n_total_steps = len(train_loader) \n",
        "\n",
        "def train_model(epochs):\n",
        "  # set model in training mode (recommended)\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "    losses = []\n",
        "    closs = 0\n",
        "    num_times = 0\n",
        "    for i, (images, labels) in enumerate(train_loader, 0):\n",
        "      images = images.to(device) # for GPU support\n",
        "      label = labels.to(device)\n",
        "\n",
        "      # Forward pass through NN\n",
        "      outputs = model(images)\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      closs+= loss.item()\n",
        "\n",
        "      # Backward pass, updating weights\n",
        "      optimizer.zero_grad() # sets gradients to 0 to prevent interference with previous epoch\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Track every 100th loss:\n",
        "      if i % 100 == 0:\n",
        "        losses.append(loss.item())\n",
        "        num_times = num_times + 1\n",
        "\n",
        "\n",
        "      # print every 1000th time\n",
        "      if i % 1000 == 0:\n",
        "        print('[%d %d] loss %.4f'% (epoch + 1, i + 1, closs/1000))\n",
        "        closs = 0\n",
        "    # Calculate the accuract and save the model state  \n",
        "    accuracy()\n",
        "\n",
        "    # Plot the loss\n",
        "    plt.plot(losses, label = 'epoch' + str(epoch))\n",
        "    plt.legend(loc = 1, mode = 'expanded', shadow = True, ncol = 2)\n",
        "  plt.show()\n",
        "\n",
        "def accuracy():\n",
        "  # set the model to evaluation mode\n",
        "  model.eval()\n",
        "  # Calculate performance\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  accuracy = 0\n",
        "  for batches in test_loader:\n",
        "    images, labels = batches\n",
        "    prediction = model(images)\n",
        "    _,prediction = nn.max(prediciton.images,1)# returns max val and index\n",
        "    total += labels.size(0)\n",
        "    correct +=(prediction == labels).sum().item()\n",
        "\n",
        "    accuracy = (correct/total) * 100\n",
        "  print('Accuracy = '+str(accuracy))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_model(2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 1, 3, 3)\n",
            "\n",
            "(16,)\n",
            "\n",
            "(16,)\n",
            "\n",
            "(16,)\n",
            "\n",
            "(32, 16, 3, 3)\n",
            "\n",
            "(32,)\n",
            "\n",
            "(32,)\n",
            "\n",
            "(32,)\n",
            "\n",
            "(120, 209024)\n",
            "\n",
            "(120,)\n",
            "\n",
            "(30, 120)\n",
            "\n",
            "(30,)\n",
            "\n",
            "(3, 30)\n",
            "\n",
            "(3,)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-ffa46f6768f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ffa46f6768f8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;31m# Forward pass through NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-ffa46f6768f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m92\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m71\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 1, 3, 3], but got 3-dimensional input of size [4, 368, 284] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hro3o1RxxrQn"
      },
      "source": [
        "#Build CNN when not using pytorch's DataLoader class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTKzGGrTZcdP"
      },
      "source": [
        "# Create classes for training data, test data \n",
        "# So far using one batch\n",
        "\n",
        "class Helper():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.i = 0\n",
        "\n",
        "    # create batches  \n",
        "    #self.train_batch = [#split X_train into batches]\n",
        "    #self.val_batch = [#validation batch]\n",
        "    #self.test_batch = [#test batch]\n",
        "\n",
        "    self.training_images = None\n",
        "    self.training_labels = None\n",
        "\n",
        "    self.val_images = None\n",
        "    self.val_labels = None\n",
        "\n",
        "    self.test_images = None\n",
        "    self.test_labels = None\n",
        "  \n",
        "\n",
        "  def get_images(self):\n",
        "\n",
        "    print(\"set up images for training, val, test\")\n",
        "\n",
        "    self.training_images = np.array(self.training_images)\n",
        "    train_len = len(self.training_images)\n",
        "\n",
        "    self.val_images = np.array(self.val_images)\n",
        "    val_len = len(self.val_images)\n",
        "\n",
        "    self.test_images = np.array(self.test_images)\n",
        "    test_len = len(self.test_images)\n",
        "\n",
        "\n",
        "\n",
        "  def next_batch(self, batch_size):\n",
        "    x = self.training_images[self.i:self.i+batch_size].reshape(batch_size,368, 284) #resize to length of batch\n",
        "    y = self.training_labels[self.i:self.i+batch_size]\n",
        "    self.i = (self.i + batch_size) % len(self.training_images)\n",
        "    return x , y \n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAoV1boOkf2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}