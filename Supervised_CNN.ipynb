{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Supervised_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10WdpyFVm5G8J5AJrHPelKDpmooAreyPw",
      "authorship_tag": "ABX9TyPjVS/DlrTNOxxF+jH6lnYw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-byrne/MADSmilestone2/blob/main/Supervised_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6L-qwlfHPf"
      },
      "source": [
        "# Supervised Learning: Clock Drawing Image Classification with Convolutional Neural Networks\n",
        "### Stacey Beck and Ian Byrne\n",
        "\n",
        "- Split data into sets of Training (x = image arrays ; y = labels), Test (~10% image arrays), and Validation (~10% of the Training). \n",
        "- Build CNN using Pytorch for Training and Test:\n",
        "  - Specify CUDA\n",
        "  - 2D convolution, Normalization (for faster training), Non-linear Activation Function (ex. RELU), Max Pooling (downsampling to reduce learned parameters).\n",
        "  - Define Layers \n",
        "  - Build Forward and backward pass\n",
        "  - Define optimizer (due to many - deep - nodes) ex) ADAM\n",
        "  - Calculate Loss (BCE)\n",
        "  - Calculate Accuracy, Precision, Recall (Confusion Matrix)\n",
        "  - Plot ROC and print Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoct7Or8ezZq",
        "outputId": "560394a9-90a2-4a5c-8c92-1f506fe0297b"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/ian-byrne/MADSmilestone2.git\n",
        "\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2\n",
        "\n",
        "# List repo contents\n",
        "#!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MADSmilestone2'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 366, done.\u001b[K\n",
            "remote: Counting objects: 100% (366/366), done.\u001b[K\n",
            "remote: Compressing objects: 100% (300/300), done.\u001b[K\n",
            "remote: Total 366 (delta 201), reused 143 (delta 60), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (366/366), 3.00 MiB | 4.53 MiB/s, done.\n",
            "Resolving deltas: 100% (201/201), done.\n",
            "/content/MADSmilestone2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtH9tfFdQepN"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "import ast\n",
        "import Images\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM_UbzqNogPC"
      },
      "source": [
        "#!rm -rf /content/MADSmilestone2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8QJAPeJfGK4"
      },
      "source": [
        "# Read round, id, label dictionary file\n",
        "\n",
        "cust_file = open(\"Data/customLabelDict.txt\", \"r\")\n",
        "#print(cust_file.readline())\n",
        "contents = cust_file.read()\n",
        "dictionary = ast.literal_eval(contents)\n",
        "cust_file.close()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV8qowHrj5Kk"
      },
      "source": [
        "def get_images_test(id_rounds):\n",
        "    counter = 0\n",
        "    store_images = []\n",
        "    #image_data = []\n",
        "    label_arr = []\n",
        "\n",
        "    for id, array in tqdm(id_rounds.items()):\n",
        "        for value in array:\n",
        "            if counter < 40:\n",
        "                url = 'https://clockimages.s3.us-west-1.amazonaws.com/NHATS_R' + str(\n",
        "                    id) + '_ClockDrawings/' + value[0] + '.tif'\n",
        "                #url = 'https://test-bucket-clockids-aicrowd.s3.us-west-1.amazonaws.com/1_'+ value[0] + '.tif'\n",
        "                # Open files and convert to work with Image in PIL\n",
        "                response = requests.get(url)  # , stream = True)\n",
        "                f = io.BytesIO(response.content)\n",
        "                im_pil = Image.open(f)\n",
        "\n",
        "                # Resize pil image files\n",
        "                #resized = im_pil.resize((im_pil.width // 9, im_pil.height // 9))\n",
        "                resized = im_pil.resize((284,368))\n",
        "                imarray1 = np.array(resized)\n",
        "\n",
        "                #imarray = np.logical_not(np.array(im)).astype(int) #bool to int, inverts values\n",
        "                #image_data.append(get_coordinates(imarray1))  # , imarray1.shape[0], imarray1.shape[1]))\n",
        "\n",
        "                #Store the np array images into a list\n",
        "                store_images.append(imarray1)\n",
        "\n",
        "                #print(\"round: \", id)\n",
        "\n",
        "                #Visualize the resized images\n",
        "                #viz_image(imarray1, resized, value[0], value[1])\n",
        "                \n",
        "                # Store corresponding labels\n",
        "                label_arr.append(value[1])\n",
        "                \n",
        "                counter += 1\n",
        "\n",
        "    return store_images, label_arr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def viz_image(image, resized, value0, value1):\n",
        "  print(\"shape: \", image.shape)\n",
        "  print(\"spid: \", value0)\n",
        "  print(\"label: \", value1)\n",
        "\n",
        "  # revert\n",
        "  im2 = Image.fromarray(np.array(resized))\n",
        "  plt.imshow(im2)\n",
        "  plt.show()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyUTQpR5kKh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fccbd90-3a71-41c9-d767-dde02ed333e4"
      },
      "source": [
        "x, y = get_images_test(dictionary)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:25<00:00,  2.60s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8XCDeIdWYWu",
        "outputId": "c120c49d-f6b2-4259-b41b-bc572f6780df"
      },
      "source": [
        "X = np.array(x)\n",
        "X.shape\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 368, 284)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "xGD41P6Q5xfr",
        "outputId": "ce784035-a0a7-4bbf-f81e-2f7af76251de"
      },
      "source": [
        "plt.imshow(X[0])"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3cc88f7810>"
            ]
          },
          "metadata": {},
          "execution_count": 140
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jwtXk7qv9-"
      },
      "source": [
        "# Build CNN Model using Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZYMHscwqu7D"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xpaSNBrNR5"
      },
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define some of the hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = .001\n",
        "#activation function\n",
        "\n",
        "# Transform PIL images to Tensors\n",
        "# Not sure if we need to transform normalize\n",
        "\n",
        "# double split to get validation and test\n",
        "ratio_val = 0.1\n",
        "ratio_test = 0.1\n",
        "\n",
        "# Get Train and Test split\n",
        "X_split, X_test, y_split, y_test = train_test_split(X, y, test_size = ratio_test, random_state = 6) \n",
        "\n",
        "# Adjust remaining ratio for even split\n",
        "ratio_remaining = 1 - ratio_test #.11111\n",
        "ratio_val_ad = ratio_val / ratio_remaining \n",
        "\n",
        "# Get Train and Val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_split, y_split, test_size = ratio_val_ad, random_state = 6)\n",
        "\n",
        "# Variables for X - X_train, X_val, X_test\n",
        "#Variables for y - y_train, y_val, y_test\n",
        "train_loader = torch.utils.data.DataLoader(X_train, batch_size = batch_size, shuffle = False)\n",
        "validate_loader = torch.utils.data.DataLoader(X_val, batch_size = batch_size, shuffle = False)\n",
        "test_loader = torch.utils.data.DataLoader(X_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "\n",
        "classes = (0, 1, 2)\n",
        "\n",
        "\n",
        "class ConvNet(n.Module):\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  def forward(self, x):\n",
        "    pass\n",
        "  \n",
        "model = ConvNet().to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Optimizer (can use SGD or ADAM)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader) # Need to define this above\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hro3o1RxxrQn"
      },
      "source": [
        "#Build CNN when not using pytorch's DataLoader class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTKzGGrTZcdP"
      },
      "source": [
        "# Create classes for training data, test data \n",
        "# So far using one batch\n",
        "\n",
        "class Helper():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.i = 0\n",
        "\n",
        "    # create batches  \n",
        "    #self.train_batch = [#split X_train into batches]\n",
        "    #self.val_batch = [#validation batch]\n",
        "    #self.test_batch = [#test batch]\n",
        "\n",
        "    self.training_images = None\n",
        "    self.training_labels = None\n",
        "\n",
        "    self.val_images = None\n",
        "    self.val_labels = None\n",
        "\n",
        "    self.test_images = None\n",
        "    self.test_labels = None\n",
        "  \n",
        "\n",
        "  def get_images(self):\n",
        "\n",
        "    print(\"set up images for training, val, test\")\n",
        "\n",
        "    self.training_images = np.array(self.training_images)\n",
        "    train_len = len(self.training_images)\n",
        "\n",
        "    self.val_images = np.array(self.val_images)\n",
        "    val_len = len(self.val_images)\n",
        "\n",
        "    self.test_images = np.array(self.test_images)\n",
        "    test_len = len(self.test_images)\n",
        "\n",
        "\n",
        "\n",
        "  def next_batch(self, batch_size):\n",
        "    x = self.training_images[self.i:self.i+batch_size].reshape(batch_size,368, 284) #resize to length of batch\n",
        "    y = self.training_labels[self.i:self.i+batch_size]\n",
        "    self.i = (self.i + batch_size) % len(self.training_images)\n",
        "    return x , y \n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAoV1boOkf2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}