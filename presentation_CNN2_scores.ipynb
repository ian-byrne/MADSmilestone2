{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "presentation_CNN2_scores.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ian-byrne/MADSmilestone2/blob/main/presentation_CNN2_scores.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa6L-qwlfHPf"
      },
      "source": [
        "# Supervised Learning: Clock Drawing Image Classification with Convolutional Neural Networks\n",
        "### Stacey Beck and Ian Byrne\n",
        "\n",
        "- Split data into sets of Training (x = image arrays ; y = labels), Test (~10% image arrays), and Validation (~10% of the Training). \n",
        "- Build CNN using Pytorch for Training and Test:\n",
        "  - Specify CUDA\n",
        "  - 2D convolution, Normalization (for faster training), Non-linear Activation Function (ex. RELU), Max Pooling (downsampling to reduce learned parameters).\n",
        "  - Define Layers \n",
        "  - Build Forward and backward pass\n",
        "  - Define optimizer (due to many - deep - nodes) ex) ADAM\n",
        "  - Calculate Loss (BCE)\n",
        "  - Calculate Accuracy, Precision, Recall (Confusion Matrix)\n",
        "  - Plot ROC and print Confusion Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nKizV2h4lMH"
      },
      "source": [
        "### Building and Training\n",
        "Architecture choices influenced from: \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/\n",
        "\n",
        "https://medium.datadriveninvestor.com/five-powerful-cnn-architectures-b939c9ddd57b\n",
        "\n",
        "https://towardsdatascience.com/how-does-sparse-convolution-work-3257a0a8fd1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKcEhDBuf5ow",
        "outputId": "ef208cbe-8dde-433a-d76e-b1537d142899"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eoct7Or8ezZq"
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/ian-byrne/MADSmilestone2.git\n",
        "# Change directory into cloned repo\n",
        "%cd MADSmilestone2\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Log111B8T-zg"
      },
      "source": [
        "!pip install torchmetrics boto3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtH9tfFdQepN"
      },
      "source": [
        "# General Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "import logging\n",
        "import os\n",
        "import boto3\n",
        "from botocore.exceptions import ClientError\n",
        "import requests\n",
        "import botocore\n",
        "import tempfile\n",
        "\n",
        "# Custom Libraries\n",
        "import Loading.load_data as ld\n",
        "import ImagePlayground.Images\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchmetrics\n",
        "\n",
        "# To Evaluate model\n",
        "from tqdm import tqdm\n",
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# To visualize model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from skimage.io import imread\n",
        "\n",
        "# To split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etc2SBZBeAkt"
      },
      "source": [
        "from utils import open_dict\n",
        "from proj_models import ResizedClocks\n",
        "from utils import collate_fn\n",
        "from utils import set_model\n",
        "from utils import accuracy\n",
        "from proj_models import ConvNet\n",
        "from utils import train_val_model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdbry9Rg6Mvl"
      },
      "source": [
        "# also import keys for aws connection\n",
        "from gdrive.MyDrive.Colab_Notebooks.clocks_aws_config import clockss3\n",
        "pubkey = clockss3['accessCode']\n",
        "seckey = clockss3['secretCode']\n",
        "client = boto3.client('s3', aws_access_key_id=pubkey, aws_secret_access_key=seckey)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5jwtXk7qv9-"
      },
      "source": [
        "# Build CNN Model using Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OOYf4s0smrq"
      },
      "source": [
        "train_batch_size = 8\n",
        "val_batch = 4\n",
        "test_batch = 1\n",
        "learning_rate = 1e-3\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1 #2*floor(3/2)\n",
        "weight_decay = 1e-5\n",
        "# Define file extension to use for new data saves\n",
        "extension_ = \"res_netfp\"\n",
        "# Define which round to get data from\n",
        "rnd = 7\n",
        "# Define model extensions for naming file (which model do we want to train on)\n",
        "model_ext = \"3\"\n",
        "# m = 'First model'\n",
        "# m = 'pre-trained'\n",
        "#m = 'pre-trained-res'\n",
        "m = 'resnet'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypf_eiS8Eghk"
      },
      "source": [
        "dictionarytr = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/tr_scor_dict_bal.txt\"\n",
        ")\n",
        "dictionaryv = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/val_scor_dict_nobal.txt\"\n",
        ")\n",
        "dictionaryts = open_dict(\n",
        "    \"/content/MADSmilestone2/Data/Dictionaries/score_dicts/tst_scor_dict_nobal.txt\"\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz4e3izRZMJx"
      },
      "source": [
        "train_set = ResizedClocks(rnd, dictionarytr[rnd], pubkey, seckey, normalize_=True)\n",
        "val_set = ResizedClocks(rnd, dictionaryv[rnd], pubkey, seckey, normalize_=True)\n",
        "test_set = ResizedClocks(rnd, dictionaryts[rnd], pubkey, seckey, normalize_=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPc4Tg8yZPiE"
      },
      "source": [
        "# Define Dataloaders for the network\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size = train_batch_size, shuffle = True, num_workers = 6, collate_fn=collate_fn) \n",
        "validate_loader = torch.utils.data.DataLoader(val_set, batch_size = val_batch, shuffle = True, num_workers = 6, collate_fn=collate_fn) #64, 8,1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kne45tSsQvFQ"
      },
      "source": [
        "# For round 10, there are some corrupt data that when batched at size 1 is not taken\n",
        "# care of by the collate function, but Nonechucks library skips the missing data and \n",
        "# moves on, replacing that missing data index with the next piece of data\n",
        "# could probably just use this in place of collate for all the loading\n",
        "if rnd == 10:\n",
        "  !pip install nonechucks\n",
        "  import nonechucks as nc\n",
        "  test_set_safe = nc.SafeDataset(test_set)\n",
        "  test_loader = torch.utils.data.DataLoader(test_set_safe, batch_size = test_batch, shuffle = False)\n",
        "else:\n",
        "  test_loader = torch.utils.data.DataLoader(test_set, batch_size = test_batch, shuffle = False, collate_fn=collate_fn)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE2oVsUSZgbl"
      },
      "source": [
        " # Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFAXymkCZ8aX",
        "outputId": "bbf38a83-2f6b-4f0f-f892-666515e4a99d"
      },
      "source": [
        "# Get model\n",
        "model = set_model(m, model_ext, device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# scheduler = lr_scheduler.StepLR(optimizer, step_size = 4, gamma=0.1)\n",
        "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience=4)\n",
        "\n",
        "accuracy_stats = {\"train\": [], \"val\": []}\n",
        "\n",
        "loss_stats = {\"train\": [], \"val\": []}"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Model training on GPU\n",
            "RESNET Model training on GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xpaSNBrNR5"
      },
      "source": [
        "train_val_model(15, model, train_loader, device, optimizer, criterion, validate_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHUjtLim2mdN"
      },
      "source": [
        "# Visualize the Training and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv3SoFHj2ssM"
      },
      "source": [
        "# Create dataframes\n",
        "train_val_acc_df = pd.DataFrame.from_dict(accuracy_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_loss_df = pd.DataFrame.from_dict(loss_stats).reset_index().melt(id_vars=['index']).rename(columns={\"index\":\"epochs\"})\n",
        "train_val_acc_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/acc{}.csv'.format(extension_), index = False)\n",
        "train_val_loss_df.to_csv('/content/gdrive/MyDrive/Colab_Notebooks/loss{}.csv'.format(extension_), index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wPK25PVfHUL"
      },
      "source": [
        "# Plot the dataframes\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
        "sns.lineplot(data=train_val_acc_df, x = \"epochs\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('Train-Val Accuracy/Epoch')\n",
        "sns.lineplot(data=train_val_loss_df, x = \"epochs\", y=\"value\", hue=\"variable\", ax=axes[1]).set_title('Train-Val Loss/Epoch')\n",
        "fig.savefig('/content/gdrive/MyDrive/Colab_Notebooks/acc_loss{}.png'.format(extension_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITYjyxcK2a9P"
      },
      "source": [
        "# Evaluate the model using Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxkr2sXqqy6x"
      },
      "source": [
        "# Calculate performance\n",
        "y_test = torch.tensor([])\n",
        "test_acc = torchmetrics.Accuracy()\n",
        "\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  #model.to(float)\n",
        "  for batches in tqdm(test_loader):\n",
        "    x_test, y_test = batches\n",
        "    x_test = x_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    y_pred = model(x_test)\n",
        "    test_acc(y_pred.cpu(), y_test.cpu())\n",
        "    total_test_acc = test_acc.compute()\n",
        "  print('test acc: ', total_test_acc)\n",
        "  test_acc.reset()\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4OOWwEWrW2-"
      },
      "source": [
        "## Create Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QROmDemn_QeY"
      },
      "source": [
        "all_pred = []\n",
        "all_preds = torch.tensor([])\n",
        "y_test = torch.tensor([])\n",
        "with torch.set_grad_enabled(False):\n",
        "  model.eval()\n",
        "  for x_test_batch, y_test_batch in tqdm(test_loader):\n",
        "    x_test_batch = x_test_batch.to(device)#.to(float).to(device)\n",
        "    y_test_pred = model(x_test_batch)\n",
        "    _, y_pred_probs = torch.max(y_test_pred, dim = 1)\n",
        "    all_pred.append(y_pred_probs.cpu().numpy())\n",
        "    all_preds = torch.cat((all_preds.cpu(), y_pred_probs.cpu()),dim = 0)\n",
        "    y_test = torch.cat((y_test, y_test_batch), dim = 0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI5PGwvU_SZq"
      },
      "source": [
        "confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, all_pred))#.rename(columns=idx2class, index=idx2class)\n",
        "sns.heatmap(confusion_matrix_df, annot=True, fmt=\".2f\", cmap='BuGn')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"label (ground truth)\")\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/model_charts/CMTX{}.png'.format(extension_))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqMBs_YS_U0U"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "#class_names = [\"Possible Dementia\", \"Likely Dementia\", \"No Dementia\"]\n",
        "class_vals = [0,1,2]\n",
        "\n",
        "cr = classification_report(y_test, all_pred, class_vals, output_dict = True)\n",
        "try:\n",
        "    cr_file = open('/content/gdrive/MyDrive/Colab Notebooks/model_charts/cr{}.txt'.format(extension_), 'wt')\n",
        "    cr_file.write(str(cr))\n",
        "    cr_file.close()\n",
        "  \n",
        "except:\n",
        "    print(\"Unable to write to file\")\n",
        "print(classification_report(y_test, all_pred, class_vals))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhz6gXGHpcMB"
      },
      "source": [
        "# Plot ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nQrbRJ9_Xoh"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "\n",
        "target= [0, 1, 2]\n",
        "\n",
        "# set plot figure size\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
        "\n",
        "# function for scoring roc auc score for multi-class\n",
        "def multiclass_roc_auc_score(y_test1, all_pred1, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test1)\n",
        "    y_test1 = lb.transform(y_test1)\n",
        "    all_pred1 = lb.transform(all_pred1)\n",
        "\n",
        "    for (idx, c_label) in enumerate(target):\n",
        "        fpr, tpr, thresholds = roc_curve(y_test1[:,idx].astype(int), all_pred1[:,idx])\n",
        "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
        "    return roc_auc_score(y_test1, all_pred1, average=average)\n",
        "\n",
        "\n",
        "print('ROC AUC score:', multiclass_roc_auc_score(y_test, all_pred))\n",
        "\n",
        "c_ax.legend()\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/model_charts/roc{}.png'.format(extension_))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8DS8yc9YMQg"
      },
      "source": [
        "# precision recall curve\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "\n",
        "# Use label_binarize to be multi-label like settings\n",
        "y = y_test.numpy()\n",
        "Y = label_binarize(y, classes=[0, 1, 2])\n",
        "Y_pred = label_binarize(all_pred, classes=[0, 1, 2])\n",
        "n_classes = Y.shape[1]\n",
        "\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(Y[:, i],\n",
        "                                                        Y_pred[:, i])\n",
        "    plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
        "    \n",
        "plt.xlabel(\"recall\")\n",
        "plt.ylabel(\"precision\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.title(\"precision vs. recall curve\")\n",
        "plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/model_charts/auc_pr{}.png'.format(extension_))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKR_i_oebCi2"
      },
      "source": [
        "## Save the GPU CNN Model\n",
        "Also includes loading on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTEwjKcbbAlJ"
      },
      "source": [
        "# Save GPU model\n",
        "model_name = 'cnn_1run.model{}'.format(extension_)\n",
        "PATH = \"/content/gdrive/MyDrive/Colab_Notebooks/{}\".format(model_name)\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm-xL-D6bcp6"
      },
      "source": [
        "\"\"\"# Load GPU model\n",
        "device = torch.device(\"cuda\")\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z8UDiRbLUG8"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    class_names = [0,1,2]\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(validate_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j].squeeze().permute(2,1))# image1.squeeze().permute(1,2,0)\n",
        "                #imshow(np.transpose(inputs.cpu().data[j], (2, 0, 1)))\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "visualize_model(model)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhrsllHHYf9H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}